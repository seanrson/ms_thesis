\begin{thebibliography}{34}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abraham et~al.(2022)Abraham, D'Oosterlinck, Feder, Gat, Geiger, Potts,
  Reichart, and Wu]{abraham2022cebab}
Eldar~D Abraham, Karel D'Oosterlinck, Amir Feder, Yair Gat, Atticus Geiger,
  Christopher Potts, Roi Reichart, and Zhengxuan Wu.
\newblock Cebab: Estimating the causal effects of real-world concepts on nlp
  model behavior.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 17582--17596, 2022.

\bibitem[Butcher(2024)]{butcher2024aligninglargelanguagemodels}
Bradley Butcher.
\newblock Aligning large language models with counterfactual dpo, 2024.
\newblock URL \url{https://arxiv.org/abs/2401.09566}.

\bibitem[Casper et~al.(2023)Casper, Davies, Shi, Gilbert, Scheurer, Rando,
  Freedman, Korbak, Lindner, Freire, Wang, Marks, Segerie, Carroll, Peng,
  Christoffersen, Damani, Slocum, Anwar, Siththaranjan, Nadeau, Michaud, Pfau,
  Krasheninnikov, Chen, Langosco, Hase, Bıyık, Dragan, Krueger, Sadigh, and
  Hadfield-Menell]{casper2023openproblemsfundamentallimitations}
Stephen Casper, Xander Davies, Claudia Shi, Thomas~Krendl Gilbert, Jérémy
  Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro
  Freire, Tony Wang, Samuel Marks, Charbel-Raphaël Segerie, Micah Carroll,
  Andi Peng, Phillip Christoffersen, Mehul Damani, Stewart Slocum, Usman Anwar,
  Anand Siththaranjan, Max Nadeau, Eric~J. Michaud, Jacob Pfau, Dmitrii
  Krasheninnikov, Xin Chen, Lauro Langosco, Peter Hase, Erdem Bıyık, Anca
  Dragan, David Krueger, Dorsa Sadigh, and Dylan Hadfield-Menell.
\newblock Open problems and fundamental limitations of reinforcement learning
  from human feedback, 2023.
\newblock URL \url{https://arxiv.org/abs/2307.15217}.

\bibitem[Chen \& Chu(2023)Chen and Chu]{chen2023causal}
Wenqing Chen and Zhixuan Chu.
\newblock Causal inference and natural language processing.
\newblock In \emph{Machine Learning for Causal Inference}, pp.\  189--206.
  Springer, 2023.

\bibitem[Dong et~al.(2023)Dong, Xiong, Goyal, Pan, Diao, Zhang, Shum, and
  Zhang]{dong2023raft}
Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang,
  Kashun Shum, and Tong Zhang.
\newblock Raft: Reward ranked finetuning for generative foundation model
  alignment.
\newblock \emph{arXiv preprint arXiv:2304.06767}, 2023.

\bibitem[Eisenstein(2022)]{eisenstein2022informativenessinvarianceperspectivesspurious}
Jacob Eisenstein.
\newblock Informativeness and invariance: Two perspectives on spurious
  correlations in natural language, 2022.
\newblock URL \url{https://arxiv.org/abs/2204.04487}.

\bibitem[Fan et~al.(2019)Fan, Jernite, Perez, Grangier, Weston, and
  Auli]{eli5_lfqa}
Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and
  Michael Auli.
\newblock {ELI5:} long form question answering.
\newblock In Anna Korhonen, David~R. Traum, and Llu{\'{\i}}s M{\`{a}}rquez
  (eds.), \emph{Proceedings of the 57th Conference of the Association for
  Computational Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2,
  2019, Volume 1: Long Papers}, pp.\  3558--3567. Association for Computational
  Linguistics, 2019.
\newblock \doi{10.18653/v1/p19-1346}.
\newblock URL \url{https://doi.org/10.18653/v1/p19-1346}.

\bibitem[Faraone(2008)]{faraone2008interpreting}
Stephen~V Faraone.
\newblock Interpreting estimates of treatment effects: implications for managed
  care.
\newblock \emph{Pharmacy and Therapeutics}, 33\penalty0 (12):\penalty0 700,
  2008.

\bibitem[Feder et~al.(2021)Feder, Oved, Shalit, and Reichart]{Feder_2021}
Amir Feder, Nadav Oved, Uri Shalit, and Roi Reichart.
\newblock Causalm: Causal model explanation through counterfactual language
  models.
\newblock \emph{Computational Linguistics}, pp.\  1–54, May 2021.
\newblock ISSN 1530-9312.
\newblock \doi{10.1162/coli_a_00404}.
\newblock URL \url{http://dx.doi.org/10.1162/coli_a_00404}.

\bibitem[Feder et~al.(2022)Feder, Keith, Manzoor, Pryzant, Sridhar,
  Wood-Doughty, Eisenstein, Grimmer, Reichart, Roberts, Stewart, Veitch, and
  Yang]{feder2022causalinferencenaturallanguage}
Amir Feder, Katherine~A. Keith, Emaad Manzoor, Reid Pryzant, Dhanya Sridhar,
  Zach Wood-Doughty, Jacob Eisenstein, Justin Grimmer, Roi Reichart,
  Margaret~E. Roberts, Brandon~M. Stewart, Victor Veitch, and Diyi Yang.
\newblock Causal inference in natural language processing: Estimation,
  prediction, interpretation and beyond, 2022.
\newblock URL \url{https://arxiv.org/abs/2109.00725}.

\bibitem[Fryer et~al.(2022)Fryer, Axelrod, Packer, Beutel, Chen, and
  Webster]{fryer2022flexibletextgenerationcounterfactual}
Zee Fryer, Vera Axelrod, Ben Packer, Alex Beutel, Jilin Chen, and Kellie
  Webster.
\newblock Flexible text generation for counterfactual fairness probing, 2022.
\newblock URL \url{https://arxiv.org/abs/2206.13757}.

\bibitem[Gat et~al.(2023)Gat, Calderon, Feder, Chapanin, Sharma, and
  Reichart]{gat2023faithfulexplanationsblackboxnlp}
Yair Gat, Nitay Calderon, Amir Feder, Alexander Chapanin, Amit Sharma, and Roi
  Reichart.
\newblock Faithful explanations of black-box nlp models using llm-generated
  counterfactuals, 2023.
\newblock URL \url{https://arxiv.org/abs/2310.00603}.

\bibitem[Gleave et~al.(2021)Gleave, Dennis, Legg, Russell, and
  Leike]{gleave2021quantifyingdifferencesrewardfunctions}
Adam Gleave, Michael Dennis, Shane Legg, Stuart Russell, and Jan Leike.
\newblock Quantifying differences in reward functions, 2021.
\newblock URL \url{https://arxiv.org/abs/2006.13900}.

\bibitem[Grimmer et~al.(2022)Grimmer, Roberts, and Stewart]{grimmer2022text}
Justin Grimmer, Margaret~E Roberts, and Brandon~M Stewart.
\newblock \emph{Text as data: A new framework for machine learning and the
  social sciences}.
\newblock Princeton University Press, 2022.

\bibitem[Gui \& Veitch(2023)Gui and Veitch]{gui2023causalestimationtextdata}
Lin Gui and Victor Veitch.
\newblock Causal estimation for text data with (apparent) overlap violations,
  2023.
\newblock URL \url{https://arxiv.org/abs/2210.00079}.

\bibitem[Hern{\'a}n(2016)]{hernan2016does}
Miguel~A Hern{\'a}n.
\newblock Does water kill? a call for less casual causal inferences.
\newblock \emph{Annals of epidemiology}, 26\penalty0 (10):\penalty0 674--680,
  2016.

\bibitem[Jin et~al.(2022)Jin, Feder, and Zhang]{jin-etal-2022-causalnlp}
Zhijing Jin, Amir Feder, and Kun Zhang.
\newblock {C}ausal{NLP} tutorial: An introduction to causality for natural
  language processing.
\newblock In Samhaa~R. El-Beltagy and Xipeng Qiu (eds.), \emph{Proceedings of
  the 2022 Conference on Empirical Methods in Natural Language Processing:
  Tutorial Abstracts}, pp.\  17--22, Abu Dubai, UAE, December 2022. Association
  for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.emnlp-tutorials.4}.
\newblock URL \url{https://aclanthology.org/2022.emnlp-tutorials.4}.

\bibitem[Joshi et~al.(2022)Joshi, Pan, and He]{joshi-etal-2022-spurious}
Nitish Joshi, Xiang Pan, and He~He.
\newblock Are all spurious features in natural language alike? an analysis
  through a causal lens.
\newblock In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.),
  \emph{Proceedings of the 2022 Conference on Empirical Methods in Natural
  Language Processing}, pp.\  9804--9817, Abu Dhabi, United Arab Emirates,
  December 2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.emnlp-main.666}.
\newblock URL \url{https://aclanthology.org/2022.emnlp-main.666}.

\bibitem[Kahneman \& Tversky(2013)Kahneman and Tversky]{kahneman2013prospect}
Daniel Kahneman and Amos Tversky.
\newblock Prospect theory: An analysis of decision under risk.
\newblock In \emph{Handbook of the fundamentals of financial decision making:
  Part I}, pp.\  99--127. World Scientific, 2013.

\bibitem[Kaushik et~al.(2020)Kaushik, Hovy, and
  Lipton]{kaushik2020learningdifferencemakesdifference}
Divyansh Kaushik, Eduard Hovy, and Zachary~C. Lipton.
\newblock Learning the difference that makes a difference with
  counterfactually-augmented data, 2020.
\newblock URL \url{https://arxiv.org/abs/1909.12434}.

\bibitem[Lambert et~al.(2024)Lambert, Pyatkin, Morrison, Miranda, Lin, Chandu,
  Dziri, Kumar, Zick, Choi, Smith, and
  Hajishirzi]{lambert2024rewardbenchevaluatingrewardmodels}
Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ~Miranda, Bill~Yuchen Lin,
  Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah~A.
  Smith, and Hannaneh Hajishirzi.
\newblock Rewardbench: Evaluating reward models for language modeling, 2024.
\newblock URL \url{https://arxiv.org/abs/2403.13787}.

\bibitem[Maas et~al.(2011)Maas, Daly, Pham, Huang, Ng, and
  Potts]{maas-EtAl:2011:ACL-HLT2011}
Andrew~L. Maas, Raymond~E. Daly, Peter~T. Pham, Dan Huang, Andrew~Y. Ng, and
  Christopher Potts.
\newblock Learning word vectors for sentiment analysis.
\newblock In \emph{Proceedings of the 49th Annual Meeting of the Association
  for Computational Linguistics: Human Language Technologies}, pp.\  142--150,
  Portland, Oregon, USA, June 2011. Association for Computational Linguistics.
\newblock URL \url{http://www.aclweb.org/anthology/P11-1015}.

\bibitem[Park et~al.(2024{\natexlab{a}})Park, Jwa, Ren, Kim, and
  Choi]{park2024offsetbias}
Junsoo Park, Seungyeon Jwa, Meiying Ren, Daeyoung Kim, and Sanghyuk Choi.
\newblock Offsetbias: Leveraging debiased data for tuning evaluators,
  2024{\natexlab{a}}.

\bibitem[Park et~al.(2024{\natexlab{b}})Park, Rafailov, Ermon, and
  Finn]{park2024disentanglinglengthqualitydirect}
Ryan Park, Rafael Rafailov, Stefano Ermon, and Chelsea Finn.
\newblock Disentangling length from quality in direct preference optimization,
  2024{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2403.19159}.

\bibitem[Sanh et~al.(2020)Sanh, Debut, Chaumond, and
  Wolf]{sanh2020distilbertdistilledversionbert}
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.
\newblock Distilbert, a distilled version of bert: smaller, faster, cheaper and
  lighter, 2020.
\newblock URL \url{https://arxiv.org/abs/1910.01108}.

\bibitem[Saxon et~al.(2024)Saxon, Holtzman, West, Wang, and
  Saphra]{saxon2024benchmarksmicroscopesmodelmetrology}
Michael Saxon, Ari Holtzman, Peter West, William~Yang Wang, and Naomi Saphra.
\newblock Benchmarks as microscopes: A call for model metrology, 2024.
\newblock URL \url{https://arxiv.org/abs/2407.16711}.

\bibitem[Shen et~al.(2023)Shen, Zheng, Zhan, Zhao, Dou, Gui, Zhang, and
  Huang]{shen2023looselipssinkships}
Wei Shen, Rui Zheng, Wenyu Zhan, Jun Zhao, Shihan Dou, Tao Gui, Qi~Zhang, and
  Xuanjing Huang.
\newblock Loose lips sink ships: Mitigating length bias in reinforcement
  learning from human feedback, 2023.
\newblock URL \url{https://arxiv.org/abs/2310.05199}.

\bibitem[Singhal et~al.(2024)Singhal, Goyal, Xu, and
  Durrett]{singhal2024longwaygoinvestigating}
Prasann Singhal, Tanya Goyal, Jiacheng Xu, and Greg Durrett.
\newblock A long way to go: Investigating length correlations in rlhf, 2024.
\newblock URL \url{https://arxiv.org/abs/2310.03716}.

\bibitem[Socher et~al.(2013)Socher, Perelygin, Wu, Chuang, Manning, Ng, and
  Potts]{socher-etal-2013-recursive}
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher~D. Manning,
  Andrew Ng, and Christopher Potts.
\newblock Recursive deep models for semantic compositionality over a sentiment
  treebank.
\newblock In \emph{Proceedings of the 2013 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  1631--1642, Seattle, Washington, USA,
  October 2013. Association for Computational Linguistics.
\newblock URL \url{https://www.aclweb.org/anthology/D13-1170}.

\bibitem[Wang et~al.(2024{\natexlab{a}})Wang, Xiong, Xie, Zhao, and
  Zhang]{ArmoRM}
Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, and Tong Zhang.
\newblock Interpretable preferences via multi-objective reward modeling and
  mixture-of-experts.
\newblock In \emph{EMNLP}, 2024{\natexlab{a}}.

\bibitem[Wang et~al.(2024{\natexlab{b}})Wang, Kulikov, Golovneva, Yu, Yuan,
  Dwivedi-Yu, Pang, Fazel-Zarandi, Weston, and
  Li]{wang2024selftaughtevaluators}
Tianlu Wang, Ilia Kulikov, Olga Golovneva, Ping Yu, Weizhe Yuan, Jane
  Dwivedi-Yu, Richard~Yuanzhe Pang, Maryam Fazel-Zarandi, Jason Weston, and
  Xian Li.
\newblock Self-taught evaluators, 2024{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2408.02666}.

\bibitem[Wang et~al.(2024{\natexlab{c}})Wang, Qiu, Yue, Guo, Zeng, Feng, and
  Shen]{wang2024surveynaturallanguagecounterfactual}
Yongjie Wang, Xiaoqi Qiu, Yu~Yue, Xu~Guo, Zhiwei Zeng, Yuhong Feng, and Zhiqi
  Shen.
\newblock A survey on natural language counterfactual generation,
  2024{\natexlab{c}}.
\newblock URL \url{https://arxiv.org/abs/2407.03993}.

\bibitem[Wang et~al.(2023)Wang, Dong, Zeng, Adams, Sreedhar, Egert, Delalleau,
  Scowcroft, Kant, Swope, and Kuchaiev]{wang2023helpsteer}
Zhilin Wang, Yi~Dong, Jiaqi Zeng, Virginia Adams, Makesh~Narsimhan Sreedhar,
  Daniel Egert, Olivier Delalleau, Jane~Polak Scowcroft, Neel Kant, Aidan
  Swope, and Oleksii Kuchaiev.
\newblock Helpsteer: Multi-attribute helpfulness dataset for steerlm, 2023.

\bibitem[Wu et~al.(2021)Wu, Ribeiro, Heer, and
  Weld]{wu2021polyjuicegeneratingcounterfactualsexplaining}
Tongshuang Wu, Marco~Tulio Ribeiro, Jeffrey Heer, and Daniel~S. Weld.
\newblock Polyjuice: Generating counterfactuals for explaining, evaluating, and
  improving models, 2021.
\newblock URL \url{https://arxiv.org/abs/2101.00288}.

\end{thebibliography}
